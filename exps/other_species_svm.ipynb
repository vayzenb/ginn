{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92fafb7-a042-4ad9-8513-9e103cecb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = '/home/vayzenbe/GitHub_Repos/GiNN'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, f'{curr_dir}/Models')\n",
    "import os, argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps,  ImageFilter\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cornet\n",
    "import model_funcs\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a6f862-4167-495c-ba42-c86747e4fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_dir = f\"{curr_dir}/Stim/\"\n",
    "weights_dir = f\"/lab_data/behrmannlab/vlad/ginn/model_weights\"\n",
    "\n",
    "im_cond = ['cropped_face', 'schematic']\n",
    "\n",
    "\n",
    "train_cond = ['imagenet_objects', 'imagenet_oneface','mixed_imagenet_vggface', 'vggface']\n",
    "test_dirs = ['facegen','monkey']\n",
    "n_classes = [601,1200]\n",
    "\n",
    "\n",
    "#train_cond = ['general']\n",
    "#n_classes = [600]\n",
    "#train_cond = ['imagenet_objects']\n",
    "#model_types = ['vggface']\n",
    "model_epochs = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "\n",
    "model_type = 'classify'\n",
    "layer =['out', 'aIT','pIT', 'V4', 'V2', 'V1']\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc123fc1-33d7-47a8-b8c1-bc6d81aa51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(dirName):\n",
    "    listOfFiles = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(dirName):\n",
    "        listOfFiles += [os.path.join(dirpath, file) for file in filenames]\n",
    "            \n",
    "    return listOfFiles    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e853f62-7f40-4709-986c-2c77cd642724",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_files = load_files(f'{stim_dir}/{test_dirs[0]}')\n",
    "im_count = len(im_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a683b0-3ed7-49f1-8f26-ad3c64fd11db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522\n"
     ]
    }
   ],
   "source": [
    "print(im_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b94fc-3840-4b43-a906-b7b39de2195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loop(model, loader):\n",
    "\n",
    "    im, label = next(iter(loader))\n",
    "\n",
    "    first_batch = True\n",
    "    for im, label in loader:\n",
    "        out = model_funcs.extract_acts(model, im)\n",
    "\n",
    "        if first_batch == True:\n",
    "            all_out = out\n",
    "            first_batch = False\n",
    "        else:\n",
    "            all_out = torch.cat((all_out, out), dim=0)\n",
    "\n",
    "    model_acts = all_out.cpu().detach().numpy()\n",
    "    \n",
    "    return model_acts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9fc53-7724-47f7-b43f-b45b89768c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for test_cond in test_dirs:\n",
    "    \n",
    "    im_dir = f'{stim_dir}/{test_cond}'\n",
    "    im_files = load_files(im_dir)\n",
    "    im_count = len(im_files)\n",
    "\n",
    "    #set up data loader for full sample\n",
    "    dataset = LoadFrames.LoadFrames(im_dir,  transform=transform)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=150, shuffle=False,num_workers = 4, pin_memory=True)\n",
    "\n",
    "    for tcn, tc  in enumerate(train_cond):\n",
    "        for ee in model_epochs:\n",
    "            base_model = model_funcs.load_model(model_type, tc, ee, weights_dir, n_classes[tcn])\n",
    "            \n",
    "            for ll in layer:\n",
    "            #load model \n",
    "                model = model_funcs.remove_layer(base_model, ll)\n",
    "\n",
    "                output, label = model_loop(model, loader)\n",
    "\n",
    "                df = pd.DataFrame(columns = ['model_type', 'train_type', 'epoch',\n",
    "                                                     'im1', 'im2', 'similarity']) \n",
    "                for ii in range(0,len(output)):\n",
    "\n",
    "                    for kk in range(ii+1,len(output)):\n",
    "\n",
    "                        sim = cos(output[ii], output[kk])\n",
    "                        sim =sim.cpu().numpy()\n",
    "\n",
    "                        trial_results = [model_type, tc, ee, label[ii], label[kk],sim]\n",
    "                        #print(trial_results)\n",
    "                        trial_results = pd.Series(trial_results, index = df.columns)\n",
    "                        df = df.append(trial_results, ignore_index=True)\n",
    "\n",
    "\n",
    "                df.to_csv(f'{curr_dir}/Results/other_species/{model_type}_{tc}_{ee}_{ll}_{test_cond}.csv', sep =\",\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
