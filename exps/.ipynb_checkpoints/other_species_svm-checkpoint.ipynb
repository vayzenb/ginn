{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fafb7-a042-4ad9-8513-9e103cecb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = '/home/vayzenbe/GitHub_Repos/GiNN'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, f'{curr_dir}/Models')\n",
    "import os, argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps,  ImageFilter\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cornet\n",
    "import model_funcs\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6f862-4167-495c-ba42-c86747e4fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_dir = f\"{curr_dir}/Stim/\"\n",
    "weights_dir = f\"/lab_data/behrmannlab/vlad/ginn/model_weights\"\n",
    "\n",
    "im_cond = ['cropped_face', 'schematic']\n",
    "\n",
    "\n",
    "train_cond = ['imagenet_objects', 'imagenet_oneface','mixed_imagenet_vggface', 'vggface']\n",
    "n_classes = [601,1200]\n",
    "\n",
    "\n",
    "#train_cond = ['general']\n",
    "#n_classes = [600]\n",
    "#train_cond = ['imagenet_objects']\n",
    "#model_types = ['vggface']\n",
    "model_epochs = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "\n",
    "model_type = 'classify'\n",
    "layer =['out', 'aIT','pIT', 'V4', 'V2', 'V1']\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b94fc-3840-4b43-a906-b7b39de2195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loop(model, loader):\n",
    "\n",
    "    im, label = next(iter(loader))\n",
    "\n",
    "    first_batch = True\n",
    "    for im, label in loader:\n",
    "        out = model_funcs.extract_acts(model, im)\n",
    "\n",
    "        if first_batch == True:\n",
    "            all_out = out\n",
    "            first_batch = False\n",
    "        else:\n",
    "            all_out = torch.cat((all_out, out), dim=0)\n",
    "\n",
    "    model_acts = all_out.cpu().detach().numpy()\n",
    "    \n",
    "    return model_acts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df672c-2757-4684-9655-aaeb91323478",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LoadFrames.LoadFrames(f'{stim_dir}/{vid}',  transform=transform)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=135, shuffle=False,num_workers = 4, pin_memory=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
