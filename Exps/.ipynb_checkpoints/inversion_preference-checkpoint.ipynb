{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face and top-heavy preferential looking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = '/home/vayzenbe/GitHub_Repos/GiNN'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, f'{curr_dir}/Models')\n",
    "import os, argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image, ImageOps,  ImageFilter\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cornet\n",
    "import model_funcs\n",
    "from load_stim import load_stim\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, mean\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_dir = f\"{curr_dir}/Stim/\"\n",
    "weights_dir = f\"/lab_data/behrmannlab/vlad/ginn/model_weights\"\n",
    "\n",
    "im_cond = ['cropped_face', 'schematic']\n",
    "\n",
    "\n",
    "train_cond = ['imagenet_objects', 'vggface']\n",
    "n_classes = [600, 600]\n",
    "\n",
    "#train_cond = ['general']\n",
    "#n_classes = [600]\n",
    "#train_cond = ['imagenet_objects']\n",
    "#model_types = ['vggface']\n",
    "epochs = [0,1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "model_type = 'classify'\n",
    "\n",
    "#model_name = 'cornet_classify'\n",
    "layer =['out', 'aIT','pIT', 'V4','V2', 'V1']\n",
    "\n",
    "scaler = transforms.Resize((224, 224))\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "relu =nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(image_name):\n",
    "    #make image grayscale\n",
    "    image_name = ImageOps.grayscale(image_name).convert(\"RGB\")\n",
    "    #image_name = image_name.filter(ImageFilter.GaussianBlur(radius=4))\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image_name = transform(image_name).unsqueeze(0)\n",
    "    #image_name = Variable(normalize(to_tensor(scaler(image_name))).unsqueeze(0))\n",
    "    image_name.cuda()\n",
    "    return image_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_top_ratio(image_name):\n",
    "    pix = np.array(image_name)\n",
    "    \n",
    "    top = pix[0:int(pix.shape[0]/2), 0:pix.shape[1]]\n",
    "    bottom = pix[int(pix.shape[0]/2):pix.shape[0], 0:pix.shape[1]]\n",
    "    \n",
    "    top_ratio = 1- (np.sum(top==255)/(np.sum(top==255) + np.sum(bottom==255)))\n",
    "    \n",
    "    return top_ratio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(dirName):\n",
    "    listOfFiles = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(dirName):\n",
    "        listOfFiles += [os.path.join(dirpath, file) for file in filenames]\n",
    "            \n",
    "    return listOfFiles    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line(df_plot, df_conds):\n",
    "    \n",
    "    plt.figure()\n",
    "    for dc in df_conds:    \n",
    "        plt.errorbar(df_plot['epoch'], df_plot[dc],  yerr=df_plot[f'{dc}_se'], label=dc)\n",
    "    \n",
    "  \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Standardized Activation/Preference\")\n",
    "    #ax.set_ylabel('Activation')\n",
    "    #print(f'{df_plot[\"model][0]}-test')\n",
    "    plt.title(f'{model_type} {df_plot[\"model\"][0]} model with {df_plot[\"stim\"][0]} stim')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{curr_dir}/Results/inversion/figures/{df_plot[\"model\"][0]}_{df_plot[\"stim\"][0]}_{layer}.png',bbox_inches='tight')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify imagenet_objects 0 /lab_data/behrmannlab/vlad/ginn/model_weights 600 cropped_face\n",
      "imagenet_objects 0 cropped_face out\n",
      "imagenet_objects 0 cropped_face aIT\n",
      "imagenet_objects 0 cropped_face pIT\n",
      "imagenet_objects 0 cropped_face V4\n",
      "imagenet_objects 0 cropped_face V2\n",
      "imagenet_objects 0 cropped_face V1\n",
      "classify imagenet_objects 1 /lab_data/behrmannlab/vlad/ginn/model_weights 600 cropped_face\n",
      "imagenet_objects 1 cropped_face out\n",
      "imagenet_objects 1 cropped_face aIT\n",
      "imagenet_objects 1 cropped_face pIT\n",
      "imagenet_objects 1 cropped_face V4\n",
      "imagenet_objects 1 cropped_face V2\n",
      "imagenet_objects 1 cropped_face V1\n",
      "classify imagenet_objects 5 /lab_data/behrmannlab/vlad/ginn/model_weights 600 cropped_face\n",
      "imagenet_objects 5 cropped_face out\n",
      "imagenet_objects 5 cropped_face aIT\n",
      "imagenet_objects 5 cropped_face pIT\n",
      "imagenet_objects 5 cropped_face V4\n",
      "imagenet_objects 5 cropped_face V2\n",
      "imagenet_objects 5 cropped_face V1\n",
      "classify imagenet_objects 10 /lab_data/behrmannlab/vlad/ginn/model_weights 600 cropped_face\n",
      "imagenet_objects 10 cropped_face out\n",
      "imagenet_objects 10 cropped_face aIT\n",
      "imagenet_objects 10 cropped_face pIT\n",
      "imagenet_objects 10 cropped_face V4\n",
      "imagenet_objects 10 cropped_face V2\n",
      "imagenet_objects 10 cropped_face V1\n",
      "classify imagenet_objects 15 /lab_data/behrmannlab/vlad/ginn/model_weights 600 cropped_face\n",
      "imagenet_objects 15 cropped_face out\n",
      "imagenet_objects 15 cropped_face aIT\n",
      "imagenet_objects 15 cropped_face pIT\n",
      "imagenet_objects 15 cropped_face V4\n",
      "imagenet_objects 15 cropped_face V2\n",
      "imagenet_objects 15 cropped_face V1\n",
      "classify imagenet_objects 20 /lab_data/behrmannlab/vlad/ginn/model_weights 600 cropped_face\n",
      "imagenet_objects 20 cropped_face out\n",
      "imagenet_objects 20 cropped_face aIT\n",
      "imagenet_objects 20 cropped_face pIT\n",
      "imagenet_objects 20 cropped_face V4\n",
      "imagenet_objects 20 cropped_face V2\n",
      "imagenet_objects 20 cropped_face V1\n",
      "classify imagenet_objects 25 /lab_data/behrmannlab/vlad/ginn/model_weights 600 cropped_face\n",
      "imagenet_objects 25 cropped_face out\n",
      "imagenet_objects 25 cropped_face aIT\n",
      "imagenet_objects 25 cropped_face pIT\n",
      "imagenet_objects 25 cropped_face V4\n",
      "imagenet_objects 25 cropped_face V2\n",
      "imagenet_objects 25 cropped_face V1\n",
      "classify imagenet_objects 30 /lab_data/behrmannlab/vlad/ginn/model_weights 600 cropped_face\n",
      "imagenet_objects 30 cropped_face out\n",
      "imagenet_objects 30 cropped_face aIT\n",
      "imagenet_objects 30 cropped_face pIT\n",
      "imagenet_objects 30 cropped_face V4\n",
      "imagenet_objects 30 cropped_face V2\n",
      "imagenet_objects 30 cropped_face V1\n",
      "classify imagenet_objects 0 /lab_data/behrmannlab/vlad/ginn/model_weights 600 schematic\n",
      "imagenet_objects 0 schematic out\n",
      "imagenet_objects 0 schematic aIT\n",
      "imagenet_objects 0 schematic pIT\n",
      "imagenet_objects 0 schematic V4\n",
      "imagenet_objects 0 schematic V2\n",
      "imagenet_objects 0 schematic V1\n",
      "classify imagenet_objects 1 /lab_data/behrmannlab/vlad/ginn/model_weights 600 schematic\n",
      "imagenet_objects 1 schematic out\n",
      "imagenet_objects 1 schematic aIT\n",
      "imagenet_objects 1 schematic pIT\n",
      "imagenet_objects 1 schematic V4\n",
      "imagenet_objects 1 schematic V2\n",
      "imagenet_objects 1 schematic V1\n",
      "classify imagenet_objects 5 /lab_data/behrmannlab/vlad/ginn/model_weights 600 schematic\n",
      "imagenet_objects 5 schematic out\n",
      "imagenet_objects 5 schematic aIT\n",
      "imagenet_objects 5 schematic pIT\n",
      "imagenet_objects 5 schematic V4\n",
      "imagenet_objects 5 schematic V2\n",
      "imagenet_objects 5 schematic V1\n",
      "classify imagenet_objects 10 /lab_data/behrmannlab/vlad/ginn/model_weights 600 schematic\n",
      "imagenet_objects 10 schematic out\n",
      "imagenet_objects 10 schematic aIT\n",
      "imagenet_objects 10 schematic pIT\n",
      "imagenet_objects 10 schematic V4\n",
      "imagenet_objects 10 schematic V2\n",
      "imagenet_objects 10 schematic V1\n",
      "classify imagenet_objects 15 /lab_data/behrmannlab/vlad/ginn/model_weights 600 schematic\n",
      "imagenet_objects 15 schematic out\n",
      "imagenet_objects 15 schematic aIT\n",
      "imagenet_objects 15 schematic pIT\n",
      "imagenet_objects 15 schematic V4\n",
      "imagenet_objects 15 schematic V2\n",
      "imagenet_objects 15 schematic V1\n",
      "classify imagenet_objects 20 /lab_data/behrmannlab/vlad/ginn/model_weights 600 schematic\n",
      "imagenet_objects 20 schematic out\n",
      "imagenet_objects 20 schematic aIT\n",
      "imagenet_objects 20 schematic pIT\n",
      "imagenet_objects 20 schematic V4\n",
      "imagenet_objects 20 schematic V2\n",
      "imagenet_objects 20 schematic V1\n",
      "classify imagenet_objects 25 /lab_data/behrmannlab/vlad/ginn/model_weights 600 schematic\n",
      "imagenet_objects 25 schematic out\n",
      "imagenet_objects 25 schematic aIT\n",
      "imagenet_objects 25 schematic pIT\n",
      "imagenet_objects 25 schematic V4\n",
      "imagenet_objects 25 schematic V2\n",
      "imagenet_objects 25 schematic V1\n",
      "classify imagenet_objects 30 /lab_data/behrmannlab/vlad/ginn/model_weights 600 schematic\n",
      "imagenet_objects 30 schematic out\n",
      "imagenet_objects 30 schematic aIT\n",
      "imagenet_objects 30 schematic pIT\n",
      "imagenet_objects 30 schematic V4\n",
      "imagenet_objects 30 schematic V2\n",
      "imagenet_objects 30 schematic V1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Extract preferences\n",
    "'''\n",
    "\n",
    "for nc,mm in enumerate(train_cond): #loop through trained model conditions\n",
    "    for cc in im_cond: #loop through test conditions\n",
    "        im_dir = f'{stim_dir}/{cc}'\n",
    "        im_files = load_files(im_dir)\n",
    "        summary_df =pd.DataFrame(columns = ['model', 'stim', 'epoch', 'upright', 'inverted', 'upright_se', 'inverted_se', 'ratio', 'ratio_se']) \n",
    "        \n",
    "        \n",
    "        for ee in epochs: #loop through model training epochs\n",
    "            top_ratios = []\n",
    "            print(model_type, mm, ee, weights_dir, n_classes[nc],cc)\n",
    "            base_model = model_funcs.load_model(model_type, mm, ee, weights_dir, n_classes[nc])\n",
    "            \n",
    "            for ll in layer:\n",
    "                model = model_funcs.remove_layer(base_model, ll)\n",
    "                \n",
    "                model.eval()            \n",
    "\n",
    "                #These are some tests on classifier trained models\n",
    "                #model = getattr(cornet, 'cornet_z')\n",
    "                #model = model(pretrained=False, map_location='gpu')\n",
    "                #checkpoint = torch.load(f\"{weights_dir}/{model_name}_{mm}_{ee}.pt\")\n",
    "                #model.load_state_dict(checkpoint['model_state_dict'])    \n",
    "                #model = make_AE(model, layer)\n",
    "\n",
    "                upright_acts = []\n",
    "                invert_acts = []\n",
    "                for ii, im_file in enumerate(im_files):\n",
    "\n",
    "\n",
    "                    with torch.no_grad():\n",
    "\n",
    "                        im = Image.open(im_file).convert(\"RGB\")\n",
    "\n",
    "                        if cc == 'schematic':\n",
    "                            top_ratios.append(calc_top_ratio(im))      \n",
    "\n",
    "                        im_invert = im.rotate(180) # rotate to invert it\n",
    "                        im = image_loader(im)\n",
    "                        im_invert = image_loader(im_invert)\n",
    "\n",
    "                        #Extract activation for upright\n",
    "                        vec = model(im) #Extract image vector\n",
    "                        vec = relu(vec) #Extract image vector\n",
    "                        vec =vec.view(vec.size(0), -1)\n",
    "                        vec = vec.cpu().detach().numpy() #Extract image vector\n",
    "                        upright_vec = pd.Series(list(chain.from_iterable(vec)))\n",
    "                        #vec = (vec - vec.mean())/vec.std() #z-score the values\n",
    "\n",
    "                        #vec = vec[vec >0]\n",
    "\n",
    "                        #Extract activation for inverted\n",
    "                        vec = model(im_invert) #Extract image vector\n",
    "                        vec = relu(vec) #Extract image vector\n",
    "                        vec =vec.view(vec.size(0), -1)\n",
    "                        vec = vec.cpu().detach().numpy() #Extract image vector\n",
    "                        invert_vec = pd.Series(list(chain.from_iterable(vec)))\n",
    "\n",
    "                        #combined_vec = upright_vec\n",
    "                        #combined_vec = combined_vec.append(invert_vec)\n",
    "\n",
    "                        #upright_vec = (upright_vec-combined_vec.mean())/combined_vec.std()\n",
    "                        #invert_vec = (invert_vec-combined_vec.mean())/combined_vec.std()\n",
    "\n",
    "                        upright_acts.append(upright_vec.mean())\n",
    "                        invert_acts.append(invert_vec.mean())\n",
    "\n",
    "\n",
    "\n",
    "                df = pd.DataFrame(list(zip(im_files, upright_acts, invert_acts)), columns = [\"image\", \"upright\", \"invert\"])\n",
    "                print(mm, ee, cc,ll)\n",
    "\n",
    "                '''\n",
    "                if cc != \"schematic\":\n",
    "                    #print(mm, ee, cc, df['upright'].mean(), df['invert'].mean(),stats.ttest_rel(df['upright'], df['invert']))\n",
    "                    pass\n",
    "                else:\n",
    "                    df['pref'] = df['upright'] /(df['upright'] + df['invert'])\n",
    "                    #df['pref'] = pd.Series(pref)\n",
    "                    df['top_ratio'] = top_ratios\n",
    "                    #print(mm, ee, cc, df['upright'].mean(), df['invert'].mean(),stats.ttest_rel(df['upright'], df['invert']), stats.pearsonr( df['pref'], df['top_ratio']))\n",
    "                    #print(mm, ee, cc, df['upright'].mean(), df['invert'].mean(),stats.ttest_rel(df['upright'], df['invert']))\n",
    "                    '''\n",
    "\n",
    "                df.to_csv(f'{curr_dir}/Results/inversion/{model_type}_{mm}_{cc}_{ee}_{ll}.csv', sep =\",\", index = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
