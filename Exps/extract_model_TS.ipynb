{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930111d-37c6-448a-8e00-70a7da980d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curr_dir = '/home/vayzenbe/GitHub_Repos/GiNN'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, f'{curr_dir}/Models')\n",
    "import os, argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps,  ImageFilter\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cornet\n",
    "import model_funcs\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "%matplotlib inline\n",
    "import LoadFrames\n",
    "from scipy import stats, spatial\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011302c0-5000-45e0-b8ed-40c1d92c808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "folder params\n",
    "'''\n",
    "\n",
    "stim_dir = f\"{curr_dir}/Stim/fmri_videos/frames\"\n",
    "weights_dir = f\"/lab_data/behrmannlab/vlad/ginn/model_weights\"\n",
    "results_dir = f\"{curr_dir}/Results/fmri_ts\"\n",
    "\n",
    "\n",
    "'''\n",
    "set model params\n",
    "'''\n",
    "model_type = 'classify'\n",
    "train_cond = ['mixed_imagenet_vggface']\n",
    "n_classes = [1200, 600]\n",
    "layer =['aIT','pIT'] #set in descending order\n",
    "epochs = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "vid = \"partly_cloudy\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "#If true, load previousl extracted acts; else extract from video\n",
    "load_acts = True \n",
    "\n",
    "'''\n",
    "fMRI and video parameters\n",
    "'''\n",
    "\n",
    "vols = 168 #volumes in the scan\n",
    "tr = 2 #TR of scan\n",
    "fix_tr =5 #first 5 volumes of the scan (10 s) were fix in the beginning\n",
    "fps = 24 # frame per second of video (how many rows go into 1 sec)\n",
    "bin_size = fps * tr # get the bin size to average by multiplying the FPS by tr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a685577-1b0f-4b49-a037-90e19eb803f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loop(model, loader):\n",
    "\n",
    "    im, label = next(iter(loader))\n",
    "\n",
    "    first_batch = True\n",
    "    for im, label in loader:\n",
    "        out = model_funcs.extract_acts(model, im)\n",
    "\n",
    "        if first_batch == True:\n",
    "            all_out = out\n",
    "            first_batch = False\n",
    "        else:\n",
    "            all_out = torch.cat((all_out, out), dim=0)\n",
    "\n",
    "    frame_acts = all_out.cpu().detach().numpy()\n",
    "    \n",
    "    return frame_acts\n",
    "\n",
    "#idx = np.argwhere(np.all(all_out[..., :] == 0, axis=0))# find columns with only zeros\n",
    "#all_out = np.delete(all_out, idx, axis=1)\n",
    "#frame_ts = (all_out - np.mean(all_out,axis = 0))/np.std(all_out, axis = 0) # standardize the unit activations; this produces NaNs right now; figure out why\n",
    "#frame_ts = frame_ts[:, ~np.isnan(frame_ts).any(axis=0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f4b49-a5ad-46a6-beeb-d5a5fab63c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample(data):\n",
    "    \"\"\"Downsample data\"\"\"\n",
    "    downsample_ts = np.empty((0, data.shape[1])) \n",
    "    \n",
    "    #Bin frame data for TS\n",
    "    for nn in range(0,len(data),bin_size):\n",
    "        temp = data[nn:(nn+bin_size),:]\n",
    "\n",
    "        downsample_ts = np.vstack((downsample_ts,np.mean(temp, axis=0)))\n",
    "\n",
    "    downsample_ts = downsample_ts[0:(vols-fix_tr),:] #extract only 168 volumes to match fmri data (credits of movie were cut)\n",
    "    \n",
    "    return downsample_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700d90f-48f4-413b-80c8-8e6d02caa6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pc(data, n_components=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Extract principal components\n",
    "    if n_components isn't set, it will extract all it can\n",
    "    \n",
    "    \"\"\"\n",
    "    pca = PCA(n_components = n_components)\n",
    "    pca.fit(data)\n",
    "    \n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f26da-03e4-40d9-9ae4-e99cdb90432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pc_n(pca, thresh):\n",
    "    '''\n",
    "    Calculate how many PCs are needed to explain X% of data\n",
    "    \n",
    "    pca - result of pca analysis\n",
    "    thresh- threshold for how many components to keep\n",
    "    '''\n",
    "\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    \n",
    "    var = 0\n",
    "    for n_comp, ev in enumerate(explained_variance):\n",
    "        var += ev #add each PC's variance to current variance\n",
    "        #print(evn, ev, var)\n",
    "\n",
    "        if var >=thresh: #once variance > than thresh, stop\n",
    "            break\n",
    "    \n",
    "    #plt.bar(range(len(explained_variance[0:n_comp])), explained_variance[0:n_comp], alpha=0.5, align='center')\n",
    "    #plt.ylabel('Variance ratio')\n",
    "    #plt.xlabel('Principal components')\n",
    "    #plt.show()\n",
    "    \n",
    "    return n_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b056e70-9e3f-490e-946e-d8ed1c29e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convolve with HRF using\n",
    "Using double-gamma with 4-sec peak\n",
    "'''\n",
    "\n",
    "def hrf(data, tr):\n",
    "    \"\"\" Return values for HRF at given times \"\"\"\n",
    "    times = np.arange(0, 30, tr)\n",
    "    # Gamma pdf for the peak\n",
    "    peak_values = gamma.pdf(times, 6)\n",
    "    # Gamma pdf for the undershoot\n",
    "    undershoot_values = gamma.pdf(times, 12)\n",
    "    # Combine them\n",
    "    values = peak_values - 0.35 * undershoot_values\n",
    "    # Scale max to 0.6\n",
    "    hrf_at_trs = values / np.max(values) * 0.6 \n",
    "    \n",
    "    \n",
    "    return np.convolve(data, hrf_at_trs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13d3fd-02c4-475c-b04d-2097c696a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_hrf(data):\n",
    "\n",
    "    conv_ts =np.zeros((data.shape))\n",
    "    for ii in range(0,data.shape[1]):\n",
    "        temp = hrf(data[:,ii],tr)\n",
    "        temp = temp[0:(vols-fix_tr)] # only grab the first 168 volumes\n",
    "        temp = (temp - np.mean(temp))/np.std(temp)\n",
    "        conv_ts[:,ii] = temp\n",
    "        \n",
    "        return conv_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff6a914-5ec1-4343-99d9-5a677967edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LoadFrames.LoadFrames(f'{stim_dir}/{vid}',  transform=transform)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=135, shuffle=False,num_workers = 4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34fcd4b7-c2c6-4e7b-9ea4-2f7b597eff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed_imagenet_vggface 0 aIT 42\n",
      "mixed_imagenet_vggface 0 pIT 54\n",
      "mixed_imagenet_vggface 1 aIT 84\n",
      "mixed_imagenet_vggface 1 pIT 98\n",
      "mixed_imagenet_vggface 5 aIT 98\n",
      "mixed_imagenet_vggface 5 pIT 106\n",
      "mixed_imagenet_vggface 10 aIT 103\n",
      "mixed_imagenet_vggface 10 pIT 108\n",
      "mixed_imagenet_vggface 15 aIT 102\n",
      "mixed_imagenet_vggface 15 pIT 107\n",
      "mixed_imagenet_vggface 20 aIT 102\n",
      "mixed_imagenet_vggface 20 pIT 107\n",
      "mixed_imagenet_vggface 25 aIT 101\n",
      "mixed_imagenet_vggface 25 pIT 107\n",
      "mixed_imagenet_vggface 30 aIT 101\n",
      "mixed_imagenet_vggface 30 pIT 107\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model loop ver. 1\n",
    "Conducts PCA on down-sample data\n",
    "'''\n",
    "\n",
    "for nc, tc in enumerate(train_cond):\n",
    "    for ee in epochs:\n",
    "        \n",
    "        model = model_funcs.load_model(model_type, tc,ee, weights_dir, n_classes[nc])\n",
    "        \n",
    "        for ll in layer:\n",
    "            model =  model_funcs.remove_layer(model, ll)\n",
    "\n",
    "            #extract acts for all frames of video\n",
    "            frame_acts = model_loop(model, loader)\n",
    "            np.save(f'fmri_data/{vid}_{model_type}_{tc}_{ee}_{ll}_allframes', frame_acts)\n",
    "\n",
    "            #downsample to scale of fmri\n",
    "            downsample_ts = down_sample(frame_acts)\n",
    "\n",
    "            #calculate components for pca\n",
    "            n_comp = calc_pc_n(extract_pc(downsample_ts),.95)\n",
    "            \n",
    "            #calculate final set of PCs\n",
    "            pca2 = extract_pc(downsample_ts, n_comp)\n",
    "            model_pcs = pca2.transform(downsample_ts) #reduce dimensionality of data using model PCs\n",
    "            #plot pc variance explained\n",
    "\n",
    "            #convolve to hrf\n",
    "            final_ts = convolve_hrf(model_pcs)\n",
    "            np.save(f'{results_dir}/{vid}_{model_type}_{tc}_{ee}_{ll}_TS', final_ts)\n",
    "            print(tc, ee,ll, n_comp)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37756281-b186-4a97-87f3-26cd193553b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model loop ver. 2\n",
    "Conducts PCA on all model data\n",
    "'''\n",
    "\n",
    "for nc, tc in enumerate(train_cond):\n",
    "    for ee in epochs:\n",
    "        \n",
    "        model = model_funcs.load_model(model_type, tc,ee, weights_dir, n_classes[nc])\n",
    "        \n",
    "        for ll in layer:\n",
    "            model =  model_funcs.remove_layer(model, ll)\n",
    "\n",
    "            #extract acts for all frames of video\n",
    "            frame_acts = model_loop(model, loader)\n",
    "            np.save(f'fmri_data/{vid}_{model_type}_{tc}_{ee}_{ll}_allframes', frame_acts)\n",
    "\n",
    "            #calculate components for pca\n",
    "            n_comp = calc_pc_n(extract_pc(frame_acts),.95)\n",
    "            \n",
    "            #calculate final set of PCs\n",
    "            pca2 = extract_pc(frame_acts, n_comp)\n",
    "            model_pcs = pca2.transform(frame_acts) #reduce dimensionality of data using model PCs\n",
    "            #plot pc variance explained\n",
    "            \n",
    "            #downsample to scale of fmri\n",
    "            downsample_ts = down_sample(model_pcs)\n",
    "\n",
    "            #convolve to hrf\n",
    "            final_ts = convolve_hrf(model_pcs)\n",
    "            np.save(f'{results_dir}/{vid}_{model_type}_{tc}_{ee}_{ll}_frame_TS', final_ts)\n",
    "            print(tc, ee,ll, n_comp)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc7277f1-f8a1-46fe-9fc0-7eeb5a92a721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_ts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e82080c31607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#conv_ts = np.load(f'fmri_data/{model_type}_{train_cond[0]}_{layer}_TS.npy')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_ts' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "#conv_ts = np.load(f'fmri_data/{model_type}_{train_cond[0]}_{layer}_TS.npy')\n",
    "\n",
    "temp= np.transpose(conv_ts[:, 0:100])\n",
    "print(temp.shape)\n",
    "\n",
    "ax = plt.figure(figsize=(12, 12))\n",
    "ax = sns.heatmap(temp)\n",
    "\n",
    "ax.set(xlabel='Time (TR)', ylabel='Component')\n",
    "plt.savefig(f'fmri_data/timeseries_example.png',bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
